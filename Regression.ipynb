{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DiwaliSales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.208.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DiwaliSales</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22428773320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+------+--------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|         State|    Zone|Occupation|Product_Category|Orders| Amount|Status|unnamed1|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+------+--------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1|23952.0|  NULL|    NULL|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3|23934.0|  NULL|    NULL|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3|23924.0|  NULL|    NULL|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop(\"Status\", \"unnamed1\")\n",
    "df_pyspark = df_pyspark.na.drop(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Cust_name: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age Group: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Product_Category: string (nullable = true)\n",
      " |-- Orders: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our goal it to predict the amount spent on Diwali, by looking at the features - Zone, Age, Gender, Marital_Status, Orders,\n",
    "# but for now we will avoid string datatypes.\n",
    "# So, first of all we will group them together:\n",
    "\n",
    "# [Age, Orders, Marital_Status] ----> New Feature ----> Independent Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n\u001b[0;32m      2\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarital_Status\u001b[39m\u001b[38;5;124m'\u001b[39m], outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndependent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\USERS\\JAKSH\\DESKTOP\\PROGRAMS\\LANGUAGES\\PYTHON\\APACHE SPARK\\VENV\\Lib\\site-packages\\pyspark\\ml\\__init__.py:31\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[0;32m     34\u001b[0m     evaluation,\n\u001b[0;32m     35\u001b[0m     feature,\n\u001b[0;32m     36\u001b[0m     fpm,\n\u001b[0;32m     37\u001b[0m     image,\n\u001b[0;32m     38\u001b[0m     recommendation,\n\u001b[0;32m     39\u001b[0m     regression,\n\u001b[0;32m     40\u001b[0m     stat,\n\u001b[0;32m     41\u001b[0m     tuning,\n\u001b[0;32m     42\u001b[0m     util,\n\u001b[0;32m     43\u001b[0m     linalg,\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[0;32m     48\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnaryTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchDistributor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m ]\n",
      "File \u001b[1;32mc:\\USERS\\JAKSH\\DESKTOP\\PROGRAMS\\LANGUAGES\\PYTHON\\APACHE SPARK\\VENV\\Lib\\site-packages\\pyspark\\ml\\image.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=['Age', 'Orders', 'Marital_Status'], outputCol='Independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+--------------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|         State|    Zone|Occupation|Product_Category|Orders| Amount|   Independent|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+--------------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1|23952.0|[28.0,1.0,0.0]|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3|23934.0|[35.0,3.0,1.0]|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3|23924.0|[35.0,3.0,1.0]|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+-------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "| Amount|   Independent|\n",
      "+-------+--------------+\n",
      "|23952.0|[28.0,1.0,0.0]|\n",
      "|23934.0|[35.0,3.0,1.0]|\n",
      "|23924.0|[35.0,3.0,1.0]|\n",
      "+-------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now as we want only two columns: Amount and Independent, so we'll select them.\n",
    "\n",
    "finalized_data = output.select('Amount', 'Independent')\n",
    "finalized_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "train_data, test_data = finalized_data.randomSplit([0.8, 0.2])\n",
    "regressor = LinearRegression(featuresCol='Independent', labelCol='Amount')\n",
    "\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([13.0648, -50.6775, -190.7537])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210.199368600932"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------+\n",
      "|Amount|   Independent|       prediction|\n",
      "+------+--------------+-----------------+\n",
      "| 206.0|[37.0,3.0,0.0]|9541.563293891495|\n",
      "| 370.0|[19.0,4.0,1.0]|9064.966324454372|\n",
      "| 407.0|[33.0,1.0,0.0]|9590.659186643155|\n",
      "| 579.0|[30.0,2.0,1.0]|9310.033716117994|\n",
      "| 582.0|[32.0,4.0,0.0]| 9425.56198135675|\n",
      "| 686.0|[50.0,2.0,0.0]|9762.082739179286|\n",
      "| 738.0|[28.0,2.0,1.0]|9283.904182929731|\n",
      "| 744.0|[28.0,2.0,1.0]|9283.904182929731|\n",
      "| 750.0|[44.0,1.0,1.0]|9543.617927999916|\n",
      "| 760.0|[19.0,4.0,1.0]|9064.966324454372|\n",
      "| 760.0|[29.0,1.0,1.0]|9347.646429087954|\n",
      "| 766.0|[23.0,4.0,0.0]|9307.979082009573|\n",
      "| 770.0|[27.0,4.0,0.0]|9360.238148386095|\n",
      "| 771.0|[25.0,4.0,1.0]|9143.354924019157|\n",
      "| 772.0|[55.0,1.0,0.0]|9878.084051714033|\n",
      "| 785.0|[21.0,3.0,0.0]|9332.527028385402|\n",
      "| 883.0|[29.0,1.0,0.0]|9538.400120266633|\n",
      "| 883.0|[29.0,3.0,0.0]|9437.045161138449|\n",
      "| 942.0|[55.0,2.0,0.0]|9827.406572149941|\n",
      "| 951.0|[20.0,1.0,1.0]|9230.063529740777|\n",
      "+------+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4287.383886815544, 27358423.2776139)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
