{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Session\n",
    "spark = SparkSession.builder.appName('DiwaliSales').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.208.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DiwaliSales</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27334976840>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Session summary\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV\n",
    "df_pyspark = spark.read.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+--------+------+--------+\n",
      "|    _c0|      _c1|       _c2|   _c3|      _c4|_c5|           _c6|             _c7|     _c8|            _c9|            _c10|  _c11|    _c12|  _c13|    _c14|\n",
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+--------+------+--------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|           State|    Zone|     Occupation|Product_Category|Orders|  Amount|Status|unnamed1|\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|     Maharashtra| Western|     Healthcare|            Auto|     1|   23952|  NULL|    NULL|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|  Andhra�Pradesh|Southern|           Govt|            Auto|     3|   23934|  NULL|    NULL|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1|   Uttar Pradesh| Central|     Automobile|            Auto|     3|   23924|  NULL|    NULL|\n",
      "|1001425|   Sudevi| P00237842|     M|     0-17| 16|             0|       Karnataka|Southern|   Construction|            Auto|     2|   23912|  NULL|    NULL|\n",
      "|1000588|     Joni| P00057942|     M|    26-35| 28|             1|         Gujarat| Western|Food Processing|            Auto|     2|   23877|  NULL|    NULL|\n",
      "|1000588|     Joni| P00057942|     M|    26-35| 28|             1|Himachal Pradesh|Northern|Food Processing|            Auto|     1|   23877|  NULL|    NULL|\n",
      "|1001132|     Balk| P00018042|     F|    18-25| 25|             1|   Uttar Pradesh| Central|         Lawyer|            Auto|     4|   23841|  NULL|    NULL|\n",
      "|1002092| Shivangi| P00273442|     F|      55+| 61|             0|     Maharashtra| Western|      IT Sector|            Auto|     1|    NULL|  NULL|    NULL|\n",
      "|1003224|   Kushal| P00205642|     M|    26-35| 35|             0|   Uttar Pradesh| Central|           Govt|            Auto|     2|   23809|  NULL|    NULL|\n",
      "|1003650|    Ginny| P00031142|     F|    26-35| 26|             1|  Andhra�Pradesh|Southern|          Media|            Auto|     4|23799.99|  NULL|    NULL|\n",
      "|1003829| Harshita| P00200842|     M|    26-35| 34|             0|           Delhi| Central|        Banking|            Auto|     1|   23770|  NULL|    NULL|\n",
      "|1000214| Kargatis| P00119142|     F|    18-25| 20|             0|  Andhra�Pradesh|Southern|         Retail|            Auto|     2|   23752|  NULL|    NULL|\n",
      "|1004035|   Elijah| P00080342|     F|    18-25| 20|             1|  Andhra�Pradesh|Southern|      IT Sector|            Auto|     2|   23730|  NULL|    NULL|\n",
      "|1001680|  Vasudev| P00324942|     M|    26-35| 26|             1|  Andhra�Pradesh|Southern|     Automobile|            Auto|     4|   23718|  NULL|    NULL|\n",
      "|1003858|     Cano| P00293742|     M|    46-50| 46|             1|  Madhya Pradesh| Central|    Hospitality|            Auto|     3|    NULL|  NULL|    NULL|\n",
      "|1000813|   Lauren| P00289942|     F|    18-25| 24|             0|  Andhra�Pradesh|Southern|           Govt|            Auto|     2|   23664|  NULL|    NULL|\n",
      "|1005447|      Amy| P00275642|     F|    46-50| 48|             1|  Andhra�Pradesh|Southern|      IT Sector|            Auto|     3|    NULL|  NULL|    NULL|\n",
      "|1001193|     Mick| P00004842|     F|    26-35| 29|             0|  Andhra�Pradesh|Southern|       Aviation|            Auto|     1|   23619|  NULL|    NULL|\n",
      "|1001883|  Praneet| P00029842|     M|    51-55| 54|             1|   Uttar Pradesh| Central|    Hospitality|            Auto|     1|   23568|  NULL|    NULL|\n",
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+--------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the above table shows it is not in proper format, i.e. it create _c0, _c1.. columns instead of using actual columns. Thus,\n",
    "\n",
    "df_pyspark = spark.read.option('header', 'true').csv('data.csv')\n",
    "\n",
    "# OR\n",
    "# df_pyspark = spark.read.csv('data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|         State|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID='1002903', Cust_name='Sanskriti', Product_ID='P00125942', Gender='F', Age Group='26-35', Age='28', Marital_Status='0', State='Maharashtra', Zone='Western', Occupation='Healthcare', Product_Category='Auto', Orders='1', Amount='23952', Status=None, unnamed1=None),\n",
       " Row(User_ID='1000732', Cust_name='Kartik', Product_ID='P00110942', Gender='F', Age Group='26-35', Age='35', Marital_Status='1', State='Andhra�Pradesh', Zone='Southern', Occupation='Govt', Product_Category='Auto', Orders='3', Amount='23934', Status=None, unnamed1=None),\n",
       " Row(User_ID='1001990', Cust_name='Bindu', Product_ID='P00118542', Gender='F', Age Group='26-35', Age='35', Marital_Status='1', State='Uttar Pradesh', Zone='Central', Occupation='Automobile', Product_Category='Auto', Orders='3', Amount='23924', Status=None, unnamed1=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head in spark is used to get first 3 rows in list format.\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: string (nullable = true)\n",
      " |-- Cust_name: string (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age Group: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- Occupation: string (nullable = true)\n",
      " |-- Product_Category: string (nullable = true)\n",
      " |-- Orders: string (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- unnamed1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Like pd.DataFrame.info() we use below method to see what col has what data type.\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reading Columns and indexing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Cust_name',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age Group',\n",
       " 'Age',\n",
       " 'Marital_Status',\n",
       " 'State',\n",
       " 'Zone',\n",
       " 'Occupation',\n",
       " 'Product_Category',\n",
       " 'Orders',\n",
       " 'Amount',\n",
       " 'Status',\n",
       " 'unnamed1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see all the columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Cust_name|\n",
      "+---------+\n",
      "|Sanskriti|\n",
      "|   Kartik|\n",
      "|    Bindu|\n",
      "|   Sudevi|\n",
      "|     Joni|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing column -> Cust_name\n",
    "df_pyspark.select('Cust_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|Cust_name|Age|\n",
      "+---------+---+\n",
      "|Sanskriti| 28|\n",
      "|   Kartik| 35|\n",
      "|    Bindu| 35|\n",
      "|   Sudevi| 16|\n",
      "|     Joni| 28|\n",
      "+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing Multiple Cols: Cust_name, Age\n",
    "df_pyspark.select(['Cust_name', 'Age']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('User_ID', 'string'),\n",
       " ('Cust_name', 'string'),\n",
       " ('Product_ID', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age Group', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('Marital_Status', 'string'),\n",
       " ('State', 'string'),\n",
       " ('Zone', 'string'),\n",
       " ('Occupation', 'string'),\n",
       " ('Product_Category', 'string'),\n",
       " ('Orders', 'string'),\n",
       " ('Amount', 'string'),\n",
       " ('Status', 'string'),\n",
       " ('unnamed1', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+----------+------+---------+------------------+------------------+--------------+-------+-----------+----------------+------------------+-----------------+------+--------+\n",
      "|summary|           User_ID|Cust_name|Product_ID|Gender|Age Group|               Age|    Marital_Status|         State|   Zone| Occupation|Product_Category|            Orders|           Amount|Status|unnamed1|\n",
      "+-------+------------------+---------+----------+------+---------+------------------+------------------+--------------+-------+-----------+----------------+------------------+-----------------+------+--------+\n",
      "|  count|             11251|    11251|     11251| 11251|    11251|             11251|             11251|         11251|  11251|      11251|           11251|             11251|            11239|     0|       0|\n",
      "|   mean| 1003004.488134388|     NULL|      NULL|  NULL|     NULL|35.421207003821884|0.4203181939383166|          NULL|   NULL|       NULL|            NULL|2.4892898409030306|9453.610857727557|  NULL|    NULL|\n",
      "| stddev|1716.1254019231633|     NULL|      NULL|  NULL|     NULL|12.754122296588871|0.4936319151275634|          NULL|   NULL|       NULL|            NULL|1.1150469616748604|5222.355869186445|  NULL|    NULL|\n",
      "|    min|           1000001|    Aaron| P00000142|     F|     0-17|                12|                 0|Andhra�Pradesh|Central|Agriculture|            Auto|                 1|            10000|  NULL|    NULL|\n",
      "|    max|           1006040|   percer|  P0099742|     M|      55+|                92|                 1|   Uttarakhand|Western|    Textile|      Veterinary|                 4|             9999|  NULL|    NULL|\n",
      "+-------+------------------+---------+----------+------+---------+------------------+------------------+--------------+-------+-----------+----------------+------------------+-----------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same as df.describe() in pandas, spark also hass .describe(). Values are NULL because it takes string into account as well.\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column\n",
    "df_pyspark = df_pyspark.withColumn('Is 28 Year Old?', df_pyspark['Age']=='28')\n",
    "\n",
    "# As Amount is string column we cannot execute below code but if it was int:\n",
    "# df_pyspark.withColumn('Amount (in k)', df_pyspark['Amount']/1000)\n",
    "\n",
    "# To create Column with default value:\n",
    "df_pyspark = df_pyspark.withColumn('AmountinK', df_pyspark['Amount'].cast('int')/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|         State|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|Is 28 Year Old?|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|           true|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|          false|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|          false|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Columns:\n",
    "df_pyspark = df_pyspark.drop('Is 28 Year Old?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column Rename\n",
    "df_pyspark = df_pyspark.withColumnRenamed('State', 'Pradesh')\n",
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Handling Null Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# na.drop takes how as argument which has two option:\n",
    "# 1. how = any -> drop a row if it contains any null value.\n",
    "# 2. how = all -> Drops rows if all the values are null.\n",
    "\n",
    "df_pyspark.na.drop(how='all').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It also has thresh as argument which will drop rows having more than k null values, given thres=k.\n",
    "df_pyspark.na.drop(thresh=2 ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(subset=[\"Zone\",\"User_ID\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+-------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount| Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+-------+--------+---------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|Married|    NULL|   23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|Married|    NULL|   23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|Married|    NULL|   23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fills every None value with \"Missing Value\"\n",
    "# df_pyspark.na.fill(\"Missing Values\").show(3)\n",
    "\n",
    "# Fills every None value with 0\n",
    "df_pyspark.na.fill(\"Married\", \"Status\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['AmountinK'], \n",
    "                  outputCols=['Amount_imputed']\n",
    "                  ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+--------------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|Amount_imputed|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+--------------+\n",
      "|1002903|Sanskriti| P00125942|     F|    26-35| 28|             0|   Maharashtra| Western|Healthcare|            Auto|     1| 23952|  NULL|    NULL|   23.952|        23.952|\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|        23.934|\n",
      "|1001990|    Bindu| P00118542|     F|    26-35| 35|             1| Uttar Pradesh| Central|Automobile|            Auto|     3| 23924|  NULL|    NULL|   23.924|        23.924|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use:\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Filter Operations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+-------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+-------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1003745|  Shivani| P00220042|     M|    26-35| 29|             0|Haryana|Northern| IT Sector|      Stationery|     2| 21563|  NULL|    NULL|   21.563|\n",
      "|1001884|   Eugene| P00339042|     F|    46-50| 47|             0|  Delhi| Central|   Textile|      Stationery|     1| 21547|  NULL|    NULL|   21.547|\n",
      "|1001804|    Ishit| P00000642|     F|    26-35| 35|             0|  Delhi| Central|    Lawyer|      Stationery|     1| 21533|  NULL|    NULL|   21.533|\n",
      "+-------+---------+----------+------+---------+---+--------------+-------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"AmountinK<=23\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|       Pradesh|    Zone|Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "|1000732|   Kartik| P00110942|     F|    26-35| 35|             1|Andhra�Pradesh|Southern|      Govt|            Auto|     3| 23934|  NULL|    NULL|   23.934|\n",
      "|1003224|   Kushal| P00205642|     M|    26-35| 35|             0| Uttar Pradesh| Central|      Govt|            Auto|     2| 23809|  NULL|    NULL|   23.809|\n",
      "|1000813|   Lauren| P00289942|     F|    18-25| 24|             0|Andhra�Pradesh|Southern|      Govt|            Auto|     2| 23664|  NULL|    NULL|   23.664|\n",
      "+-------+---------+----------+------+---------+---+--------------+--------------+--------+----------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Works for String Values too.\n",
    "df_pyspark.filter(\"Occupation=='Govt'\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|Cust_name|Product_ID|\n",
      "+---------+----------+\n",
      "|   Kartik| P00110942|\n",
      "|    Bindu| P00118542|\n",
      "|     Joni| P00057942|\n",
      "+---------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View Only Cust_name and Prodcut_ID for married users.\n",
    "df_pyspark.filter(\"Marital_Status==1\").select([\"Cust_name\", \"Product_ID\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+------+------+--------+---------+\n",
      "|User_ID|Cust_name|Product_ID|Gender|Age Group|Age|Marital_Status|         Pradesh|    Zone|     Occupation|Product_Category|Orders|Amount|Status|unnamed1|AmountinK|\n",
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+------+------+--------+---------+\n",
      "|1000588|     Joni| P00057942|     M|    26-35| 28|             1|         Gujarat| Western|Food Processing|            Auto|     2| 23877|  NULL|    NULL|   23.877|\n",
      "|1000588|     Joni| P00057942|     M|    26-35| 28|             1|Himachal Pradesh|Northern|Food Processing|            Auto|     1| 23877|  NULL|    NULL|   23.877|\n",
      "|1006040|     Pond| P00271242|     F|    26-35| 28|             1|           Delhi| Central|      IT Sector|Footwear & Shoes|     1| 20960|  NULL|    NULL|    20.96|\n",
      "+-------+---------+----------+------+---------+---+--------------+----------------+--------+---------------+----------------+------+------+------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding multiple conditions and & | ~ not operation.\n",
    "\n",
    "df_pyspark.filter((df_pyspark['Age']=='28') \n",
    "                & (df_pyspark['Marital_Status']==1) & \n",
    "                (~(df_pyspark['Occupation']=='Govt') |\n",
    "                 ~(df_pyspark['Pradesh']=='Delhi'))\n",
    "                 ).show(3)\n",
    "\n",
    "# Above basically looks for 28 year old married person who is either not occupied in the Govt Sector or not from Delhi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Groupby & Aggregate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|Gender|   sum(AmountinK)|\n",
      "+------+-----------------+\n",
      "|     F|74335.85300000003|\n",
      "|     M|31913.27599999998|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by gender that in total spend most amount\n",
    "df_pyspark.groupby(\"Gender\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|Age Group|   avg(AmountinK)|\n",
      "+---------+-----------------+\n",
      "|    18-25|9.175482703565718|\n",
      "|    26-35| 9.38415371063643|\n",
      "|     0-17|9.120449324324326|\n",
      "|    46-50|9.367084435401823|\n",
      "|    51-55|9.953586746987956|\n",
      "|    36-45|9.699953569864219|\n",
      "|      55+|9.557346604215455|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Age wise average spent amount.\n",
    "df_pyspark.groupby(\"Age Group\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+\n",
      "|Age Group|Gender|count|\n",
      "+---------+------+-----+\n",
      "|    51-55|     F|  554|\n",
      "|    18-25|     M|  574|\n",
      "|     0-17|     F|  162|\n",
      "|    46-50|     M|  291|\n",
      "|    18-25|     F| 1305|\n",
      "|      55+|     M|  155|\n",
      "|      55+|     F|  273|\n",
      "|    36-45|     M|  705|\n",
      "|    26-35|     F| 3271|\n",
      "|     0-17|     M|  134|\n",
      "|    36-45|     F| 1581|\n",
      "|    51-55|     M|  278|\n",
      "|    26-35|     M| 1272|\n",
      "|    46-50|     F|  696|\n",
      "+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of age group and Gender\n",
    "df_pyspark.groupby([\"Age Group\",\"Gender\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|Product_ID|max(AmountinK)|\n",
      "+----------+--------------+\n",
      "| P00180642|        23.546|\n",
      "| P00146342|        20.678|\n",
      "| P00026042|         19.69|\n",
      "+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 Products\n",
    "df_pyspark.groupby(\"Product_ID\").max().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    sum(AmountinK)|\n",
      "+------------------+\n",
      "|106249.12899999987|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding Sum of total revenue:\n",
    "df_pyspark.agg({'AmountinK':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
